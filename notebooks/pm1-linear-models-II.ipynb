{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Using logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we're gonna look at some iron ore data to try to predict the ore deposits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/iron_ore_study.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're gonna add a variable defining if a sample comes from an ore deposit or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits from oscar Fe>60%, SiO2<9, Al2O3<2, P<0.08\n",
    "split_points = [\n",
    "    ('FE', 60, [False, True]),\n",
    "    ('SIO2', 9, [True, False]),\n",
    "    ('AL2O3', 2, [True, False]),\n",
    "    ('P', 0.08, [True, False]),  \n",
    "]\n",
    "\n",
    "# It's ore if everything is True\n",
    "df['is_ore'] = np.vstack([\n",
    "    pd.cut(df[elem], bins=[0, split, 100], labels=is_ore)\n",
    "    for elem, split, is_ore in split_points\n",
    "]).sum(axis=0) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new variable *is_ore* is categorical, with two possible values: True or False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot('is_ore', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at our split on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df.iloc[::5], hue='is_ore', plot_kws={'alpha': 0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the data individually while looking at the marginal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot('SIO2', 'FE', df, joint_kws={'alpha': 0.4, 'marker': '.'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot('P', 'FE', df, joint_kws={'alpha': 0.4, 'marker': '.'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "You can think of the logistic function as a function that takes a real number (as comes out of the linear regression) and 'squashes' it into a 0, 1 label. It's defined as\n",
    "\n",
    "$$\n",
    "g(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):    \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "zs = np.linspace(-10, 10)\n",
    "logistic_data = pd.DataFrame( \n",
    "    {'z': zs, 'logistic': logistic(zs)}\n",
    ")\n",
    "logistic_data.plot('z', 'logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can generate a linear model fit with one parameter - call it $f$:\n",
    "\n",
    "$$\n",
    "z = f(x) = a + b x\n",
    "$$\n",
    "\n",
    "where $a$ is the intercept, $b$ the coefficient and $x$ is the input features. Then we get label predictions\n",
    "\n",
    "$$\n",
    "\\mathrm{label} = g(f(x)) > threshold\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "Let's pretend we have a crappy sensor which only measures Al. Can we still make good predictions of ore/not ore using just this feature?\n",
    "\n",
    "We should look at transforming our aluminium data so that we go from (0, inf) -> (-inf, inf). We'll do this in a hacky sense by using a log function but we should really use a log-ratio transform here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['AL2O3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(np.log(df['AL2O3']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do this using a scikit-learn pipeline - this lets us chain transformations and predictions into one object which makes life a lot easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, pipeline, linear_model\n",
    "\n",
    "# Make up our pipeline where we transform the aluminium first to make it more gaussian!\n",
    "regressor = pipeline.Pipeline([\n",
    "    ('transform', preprocessing.FunctionTransformer(np.log, validate=True)),\n",
    "    ('model', linear_model.LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we map the data into the `y ~ f(X)` format that scikit-learn wants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['AL2O3']]\n",
    "y = df['is_ore']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fitting the model is as simple as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've fitted the model we can make predictions straight away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = pd.DataFrame(\n",
    "    {'test_al2o3_values': [0.5, 1, 2, 3, 4]} # Are these values ore?\n",
    ")  \n",
    "regressor.predict(predict_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what's going on in a bit more depth, we can pull the coefficients out of the scikit-learn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = regressor.named_steps.model\n",
    "model.intercept_, model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can evaulate our logistic function for our test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.predict_proba(predict_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these in hand lets generate some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1)\n",
    "\n",
    "# Some aluminium values to predict from\n",
    "al_compositions = pd.DataFrame(\n",
    "    {'test_al2o3_values': np.linspace(0.1, 3)}\n",
    ")\n",
    "\n",
    "# An offset to stop everything plotting on top of everything else\n",
    "offset = 0.02\n",
    "\n",
    "# shows predictions given contents\n",
    "predictions = regressor.predict(al_compositions)\n",
    "ax.plot(al_compositions, predictions + offset, '.', alpha=0.7, label='predicted (+ offset)')  \n",
    "\n",
    "# shows measured values plus jitter\n",
    "jitter = np.random.normal(scale=0.01, size=len(df))\n",
    "ax.plot(df['AL2O3'], df['is_ore'] + jitter - offset, '.', alpha=0.1, label='measured (+ jitter - offset)')\n",
    "\n",
    "# shows logistic function fitted from regressor\n",
    "ax.plot(al_compositions, regressor.predict_proba(al_compositions)[:, 1], '--', label='fitted logistic function')\n",
    "\n",
    "# Generate the logistic curve showing the location of \n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.legend()\n",
    "ax.set_title('Logistic regression with scikit-learn')\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also try this using one of the other variables!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring model performance\n",
    "\n",
    "Looking at the previous plot, it feels like we won't get everything right! How can we get a feeling for the model performance? What are some of the issues that we might need to take into account?\n",
    "\n",
    "Splitting our data between a training and a test set is a good way to start validating our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train on just the training set, predict on the test set and see how we do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'AL2O3': X_test['AL2O3'],\n",
    "    'is_ore_actual': y_test,\n",
    "    'is_ore_predicted': y_predict,\n",
    "    'count': 1\n",
    "})\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the number of false positives and false negatives using pivot_table from last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = results.pivot_table(values='count', index='is_ore_actual', columns='is_ore_predicted', aggfunc='sum')\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and plot with sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion, cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we can unstack the array into true and false negatives and positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can convert these values to fractions of the total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_neg, false_neg, true_pos, false_pos = confusion.unstack()/confusion.unstack().sum()\n",
    "true_neg, false_neg, true_pos, false_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When are true positives and false positives important?\n",
    "\n",
    "## Threshold\n",
    "\n",
    "We haven't done anything with the threshold yet. By default scikit-learn uses 0.5 in *predict*, but how should we pick the value for this?\n",
    "\n",
    "We've already got a confusion matrix - we can take the ratio of the true_positive vs the false_positive rates and compare the two "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = np.diag(confusion)\n",
    "incorrect = np.diag(np.roll(confusion, 1, axis=1))\n",
    "correct, incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_correct = correct.sum()\n",
    "total_incorrect = incorrect.sum()\n",
    "total_correct, total_incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can plot this for each threshold. We need a way of adjusting the class weights in the model. Scikit-learn doesn't let you specify a threshold directly but "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_with_class_threshold(threshold):\n",
    "    \"Fit a logistic regression to get an ROC value for a given threshold\"\n",
    "    # Transform our threshold into class weights\n",
    "    class_weights = {True: threshold, False: 1 - threshold}\n",
    "    \n",
    "    # Make a regressor\n",
    "    regressor = pipeline.Pipeline([\n",
    "        ('transform', preprocessing.FunctionTransformer(np.log, validate=True)),\n",
    "        ('model', linear_model.LogisticRegression(class_weight=class_weights))\n",
    "    ])\n",
    "    \n",
    "    # Fit it\n",
    "    regressor.fit(X_train, y_train)\n",
    "    \n",
    "    # Make some predictions, see how we did\n",
    "    results = pd.DataFrame({\n",
    "        'AL2O3': X_test['AL2O3'],\n",
    "        'is_ore_actual': y_test,\n",
    "        'is_ore_predicted': regressor.predict(X_test),\n",
    "        'count': 1\n",
    "    })\n",
    "    confusion = results.pivot_table(\n",
    "        values='count', \n",
    "        index='is_ore_actual', \n",
    "        columns='is_ore_predicted', \n",
    "        aggfunc='sum')\n",
    "    true_neg, false_neg, true_pos, false_pos = confusion.unstack()\n",
    "    correct = np.diag(confusion)\n",
    "    incorrect = np.diag(np.roll(confusion, 1, axis=1))\n",
    "    \n",
    "    # Return results as a dictionary\n",
    "    return {\n",
    "        'threshold': threshold,\n",
    "        'total_correct': correct.sum(), \n",
    "        'total_incorrect': incorrect.sum(), \n",
    "        'true_negative': true_neg,\n",
    "        'false_negative': false_neg,\n",
    "        'true_positive': true_pos,\n",
    "        'false_positive': false_pos,\n",
    "        'accuracy': correct.sum()/(correct.sum() + incorrect.sum()),\n",
    "        'precision': true_pos/(true_pos + false_pos),\n",
    "        'recall': true_pos/(true_pos + false_neg)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate results using our metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_with_class_threshold(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_with_class_threshold(0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate over all our thresholds and see what does the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame.from_records(\n",
    "    [fit_with_class_threshold(t) for t in np.linspace(0.1, 0.9)],\n",
    "    index='threshold'\n",
    ")\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A high accuracy means few labels were incorrectly predicted. It is not always a good indicator of the quality of a binary classification since it doesn't account for any imbalance between the categories.\n",
    "\n",
    "That's why precision and recall are always measured as well. A high precision means a low false positive rate, a high recall means a low false negative rate. As scikit-learn [puts it](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html):\n",
    "\n",
    "\"A system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the [test] labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the [test] labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot(y=['total_correct', 'total_incorrect'])\n",
    "results.plot(y=['false_negative', 'true_negative'])\n",
    "results.plot(y=['false_positive', 'true_positive'])\n",
    "results.plot(y=['accuracy', 'precision', 'recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn implements a lot of those metrics already, see for instance:\n",
    "\n",
    "* [Confusion matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)\n",
    "* [Accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)\n",
    "* [Precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score)\n",
    "* [Recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n",
    "\n",
    "Another useful tool to assess the performance of a classifier is the Receiver Operating Characteristic (ROC) metric. It is a curve showing false positive rate vs. true positive rate. Ideally, the curve for the classifier should be as close as possible to the top-left corner (i.e., lots of true positive and few false positive). The diagonal line basically means a random classifier. Measuring the area below the curve turns the curve into a single value: close to 1 is better, 0.5 indicates a random classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, regressor.predict_proba(X_test)[:, 1])\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)'%roc_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension: Modelling with statsmodels\n",
    "\n",
    "For what it's worth we can also generate these using [statsmodels](https://www.statsmodels.org/stable/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to handle preprocessing ourselves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tools import add_constant\n",
    "\n",
    "def preprocess(x):\n",
    "    \"Our preprocessing pipeline for Al2O3\"\n",
    "    return add_constant(np.log(x)) # add_constant adds an intercept to the fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statsmodels uses stats jargon \n",
    "- endog -> endogenous variable -> y\n",
    "- exog -> exogenous variable -> X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endog = df.is_ore\n",
    "exog = preprocess(df['AL2O3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model is pretty similar though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.Logit(endog, exog)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statsmodels does a bit more statistical testing/automated confidence intervals for us at the cost of having to manage crossvalidation etc ourselves. Depending on what you're trying to achieve this could be a viable way to go.\n",
    "\n",
    "As before we can immediately make some predictions - statsmodels gives us the value of the logistic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(results.params, preprocess(X_test['AL2O3']))\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and compare to scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.predict(X_test[['AL2O3']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare the logistic values get slightly different answers here - probably down to the solver used under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' statsmodels:', model.predict(results.params, preprocess(X_test['AL2O3'])))\n",
    "print('scikit-learn:', regressor.predict_proba(X_test[['AL2O3']])[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate the same plot again though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1)\n",
    "\n",
    "# Some aluminium values to predict from\n",
    "al_compositions = np.linspace(0.1, 3)\n",
    "\n",
    "# An offset to stop everything plotting on top of everything else\n",
    "offset = 0.02\n",
    "\n",
    "# shows predictions given contents\n",
    "predictions = model.predict(results.params, preprocess(al_compositions)) > 0.5\n",
    "ax.plot(al_compositions, predictions + offset, '.', alpha=0.7, label='predicted (+ offset)')  \n",
    "\n",
    "# shows measured values plus jitter\n",
    "jitter = np.random.normal(scale=0.01, size=len(df))\n",
    "ax.plot(df['AL2O3'], df['is_ore'] + jitter - offset, '.', alpha=0.1, label='measured (+ jitter - offset)')\n",
    "\n",
    "# shows logistic function fitted from regressor\n",
    "ax.plot(al_compositions, model.predict(results.params, preprocess(al_compositions)), '--', label='fitted logistic function')\n",
    "\n",
    "# Generate the logistic curve showing the location of \n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.legend()\n",
    "ax.set_title('Logistic regression with statsmodels')\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Importance of validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we want to see the importance of splitting our dataset into a training set and a test set. To make things easier to understand, we're gonna work with synthetic data generated using NumPy.\n",
    "\n",
    "First, we need to make our results reproducible by setting a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate the synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things a little more interesting, we're gonna use a sinusoidal function with some random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 120\n",
    "x = np.random.uniform(0, 1, n_samples)\n",
    "noise = np.random.normal(0, 0.1, n_samples)\n",
    "y = np.sin(np.pi*x) + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=x, y=y, fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fit a polynomial model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a bell shape like that a simple linear model won't do, we need a polynomial model. To do that in scikit-learn, we build a pipeline combining *PolynomialFeatures*, which defines the degree of the polynomial, with *LinearRegression* to perform the actual regression.\n",
    "\n",
    "Fill the pipeline to get a polynomial model of degree 20 (let's start high, because more degrees means a more complex model and that's always a good start, right?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(0, 1, 1000)\n",
    "y_plot = model.predict(x_plot.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.regplot(x=x, y=y, fit_reg=False, ax=ax)\n",
    "sns.lineplot(x=x_plot, y=y_plot, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And at how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Check a model with fewer degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure that 20 isn't too much, let's check a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(0, 1, 1000)\n",
    "y_plot = model.predict(x_plot.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.regplot(x=x, y=y, fit_reg=False, ax=ax)\n",
    "sns.lineplot(x=x_plot, y=y_plot, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's really bad... We're not capturing the bell shape at all, and the score and error show it clearly. The problem is that our data are clearly non-linear, and a linear model is not capable of capturing this pattern. So our model is **under-fitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check again with the lowest degree capable of capturing this pattern: 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(0, 1, 1000)\n",
    "y_plot = model.predict(x_plot.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.regplot(x=x, y=y, fit_reg=False, ax=ax)\n",
    "sns.lineplot(x=x_plot, y=y_plot, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better than the linear model, but worse than a degree of 20, so a high degree must be better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 4: Use a test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, maybe we shouldn't train and test on the same data. Let's check the impact of that.\n",
    "\n",
    "First, we need to split our dataset into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what happens with a degree of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(20), LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train.reshape(-1, 1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(0, 1, 1000)\n",
    "y_plot = model.predict(x_plot.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.regplot(x=x, y=y, fit_reg=False, ax=ax)\n",
    "sns.lineplot(x=x_plot, y=y_plot, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x_test.reshape(-1, 1), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now with a degree of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(2), LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train.reshape(-1, 1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(0, 1, 1000)\n",
    "y_plot = model.predict(x_plot.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.regplot(x=x, y=y, fit_reg=False, ax=ax)\n",
    "sns.lineplot(x=x_plot, y=y_plot, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x_test.reshape(-1, 1), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, our validation metrics are better with the lower degree now... But that was only one possible splitting, and we don't have so many data, maybe testing it again will lead to different results.\n",
    "\n",
    "Instead of doing it manually, let's use scikit-learn's cross-validation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(20), LinearRegression())\n",
    "score = cross_val_score(model, x.reshape(-1, 1), y, scoring='neg_mean_squared_error', cv=100)\n",
    "-score.mean(), score.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(2), LinearRegression())\n",
    "score = cross_val_score(model, x.reshape(-1, 1), y, scoring='neg_mean_squared_error', cv=100)\n",
    "-score.mean(), score.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a degree 2 is clearly a better choice. The problem is that a more complex model can fit more complex patterns, and it's trying to identify patterns in the noise as well. Our model with a degree of 20 is **over-fitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Choose the right degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The now the question become how to choose the best degree for the problem?\n",
    "\n",
    "One easy way to identify that degree is to add the cross-validation into a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = list(range(1, 31))\n",
    "n_cv = 100\n",
    "scores = []\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    score = cross_val_score(model, x.reshape(-1, 1), y, scoring='neg_mean_squared_error', cv=n_cv)\n",
    "    scores.append(-score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=degrees, y=scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at the degree with the smallest error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees[np.argmin(scores)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out scikit-learn can look for and give us directly the best model. To do so, we just need to perform a grid search over the parameter *degree* of our pipeline. For each value of degree we want to test, scikit-learn is gonna perform 100 cross-validations with a given score, and use that to pick the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=[('polynomial', PolynomialFeatures()),\n",
    "                        ('regression', LinearRegression())])\n",
    "param_grid = [{'polynomial__degree': degrees}]\n",
    "scoring = 'neg_mean_squared_error'\n",
    "n_cv = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(model, param_grid, scoring=scoring, cv=n_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.fit(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the search led to the right degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can directly access the model to make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(0, 1, 1000)\n",
    "y_plot = search.best_estimator_.predict(x_plot.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.regplot(x=x, y=y, fit_reg=False, ax=ax)\n",
    "sns.lineplot(x=x_plot, y=y_plot, ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
