{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Using logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if using jupyterhub\n",
    "# %pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we're gonna look at some iron ore data to try to predict the ore deposits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/iron_ore_study.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're gonna add a variable defining if a sample comes from an ore deposit or not. In a labelled dataset for supervised learning, this information may aleady be present, and we would be able to use it to build a model to predict whether a sample is an ore deposit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits from oscar Fe>60%, SiO2<9, Al2O3<2, P<0.08\n",
    "split_points = [\n",
    "    ('FE', 60, [False, True]),\n",
    "    ('SIO2', 9, [True, False]),\n",
    "    ('AL2O3', 2, [True, False]),\n",
    "    ('P', 0.08, [True, False]),  \n",
    "]\n",
    "\n",
    "# It's ore if all 4 conditions are True\n",
    "df['is_ore'] = np.vstack([\n",
    "    pd.cut(df[elem], bins=[0, split, 100], labels=is_ore)\n",
    "    for elem, split, is_ore in split_points\n",
    "]).sum(axis=0) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new variable *is_ore* is categorical, with two possible values: True or False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='is_ore', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at our split on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df.iloc[::5], hue='is_ore', plot_kws={'alpha': 0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the data individually while looking at the marginal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='SIO2', y='FE', data=df, joint_kws={'alpha': 0.4, 'marker': '.'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='P', y='FE', data=df, joint_kws={'alpha': 0.4, 'marker': '.'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "You can think of the logistic function as a function that takes a real number (as comes out of the linear regression) and 'squashes' it into a 0, 1 label. It's defined as\n",
    "\n",
    "$$\n",
    "g(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):    \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "zs = np.linspace(-10, 10)\n",
    "logistic_data = pd.DataFrame( \n",
    "    {'z': zs, 'logistic': logistic(zs)}\n",
    ")\n",
    "logistic_data.plot('z', 'logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can generate a linear model fit with one parameter - call it $f$:\n",
    "\n",
    "$$\n",
    "z = f(x) = a + b x\n",
    "$$\n",
    "\n",
    "where $a$ is the intercept, $b$ the coefficient and $x$ is the input features. Then we get label predictions\n",
    "\n",
    "$$\n",
    "\\mathrm{label} = g(f(x)) > threshold\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of looking at this, if P is the probabiliy of ore\n",
    "$$\n",
    "P = \\frac{1}{1+ e^{-z}} \\\\\n",
    "z = a + bx\n",
    "$$\n",
    "Rearrange to get,\n",
    "$$\n",
    "log(\\frac{P}{1 - P}) = a + bx\n",
    "$$\n",
    "\n",
    "LHS is called 'Log-Odds' and so, we're modelling the log-odds with a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "Let's pretend we have a crappy sensor which only measures Al. Can we still make good predictions of ore/not ore using just this feature?\n",
    "\n",
    "We should look at transforming our aluminium data so that we go from (0, inf) -> (-inf, inf). We'll do this in a hacky sense by using a log function but we should really use a log-ratio transform here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['AL2O3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(np.log(df['AL2O3']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do this using a scikit-learn pipeline - this lets us chain transformations and predictions into one object which makes life a lot easier. Using a pipeline also means we can treat our pipeline object as a scikit-learn model, and fit and predict in similar ways, `regressor.fit(X,y)` and `regressor.predict(X)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, pipeline, linear_model\n",
    "\n",
    "# Make up our pipeline where we transform the aluminium first to make it more gaussian!\n",
    "regressor = pipeline.Pipeline([\n",
    "    ('transform', preprocessing.FunctionTransformer(np.log, validate=True)),\n",
    "    ('model', linear_model.LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we map the data into the `y ~ f(X)` format that scikit-learn wants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['AL2O3']]\n",
    "y = df['is_ore']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fitting the model is as simple as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've fitted the model we can make predictions straight away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = pd.DataFrame(\n",
    "    {'test_al2o3_values': [0.5, 1, 2, 3, 4]} # Are these values ore?\n",
    ")  \n",
    "regressor.predict(predict_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what's going on in a bit more depth, we can pull the coefficients out of the scikit-learn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = regressor.named_steps.model\n",
    "model.intercept_, model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can evaulate our logistic function for our test values. `predict_proba()` comes in handy when we're not just interested in the prediction of ore/not-ore, but also how likely is it to be ore/not-ore. High probablility values indicates a strong evidence in the data supporting the model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.predict_proba(predict_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these in hand lets generate some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1)\n",
    "\n",
    "# Some aluminium values to predict from\n",
    "al_compositions = pd.DataFrame(\n",
    "    {'test_al2o3_values': np.linspace(0.1, 3)}\n",
    ")\n",
    "\n",
    "# An offset to stop everything plotting on top of everything else\n",
    "offset = 0.02\n",
    "\n",
    "# shows predictions given contents\n",
    "predictions = regressor.predict(al_compositions)\n",
    "ax.plot(al_compositions, predictions + offset, '.', alpha=0.7, label='predicted (+ offset)')  \n",
    "\n",
    "# shows measured values plus jitter\n",
    "jitter = np.random.normal(scale=0.01, size=len(df))\n",
    "ax.plot(df['AL2O3'], df['is_ore'] + jitter - offset, '.', alpha=0.1, label='measured (+ jitter - offset)')\n",
    "\n",
    "# shows logistic function fitted from regressor\n",
    "ax.plot(al_compositions, regressor.predict_proba(al_compositions)[:, 1], '--', label='fitted logistic function')\n",
    "\n",
    "# Generate the logistic curve showing the location of \n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.legend()\n",
    "ax.set_title('Logistic regression with scikit-learn')\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatterplot data points (in orange) show the original AL2O3 data, ranging between 0 to 4. Some of these samples is ore (plotted at y axis = 1) and some are not ore (plotted at y axis = 0). The logistic model fit then tells us low values of AL2O3 (AL2O3 approx less than 1.5) are predicted to be ore, and high values of AL2O3 (> 1.5) are predicted to be not ore. This model is not always accurate, but does provide the right predictions for very low and very high values of AL3O3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: You can also try this using one of the other variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, pipeline, linear_model\n",
    "\n",
    "# Make up a model or pipeline\n",
    "regressor =  \n",
    "\n",
    "# Set up inputs\n",
    "X = \n",
    "y = df['is_ore']\n",
    "\n",
    "# Fit the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring model performance\n",
    "\n",
    "Looking at the previous plot, it feels like we won't get everything right! How can we get a feeling for the model performance? What are some of the issues that we might need to take into account?\n",
    "\n",
    "Splitting our data between a training and a test set is a good way to start validating our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[['AL2O3']]\n",
    "y = df['is_ore']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train on just the training set, predict on the test set and see how we do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'AL2O3': X_test['AL2O3'],\n",
    "    'is_ore_actual': y_test,\n",
    "    'is_ore_predicted': y_predict,\n",
    "    'count': 1\n",
    "})\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the number of false positives and false negatives using pivot_table from last week (rows are actual T/F and columns are predicted T/F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = results.pivot_table(values='count', index='is_ore_actual', columns='is_ore_predicted', aggfunc='sum')\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and plot with sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion, cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we can unstack the array into true and false negatives and positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can convert these values to fractions of the total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_neg, false_neg, false_pos, true_pos = confusion.unstack()/confusion.unstack().sum()\n",
    "true_neg, false_neg, false_pos, true_pos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- True positives - Model predicts true, and model is true/accurate\n",
    "- False positives - Model predicts true, and model is false/inaccurate\n",
    "- True negatives - Model predicts false, and model is true/accurate\n",
    "- False negatives - Model predicts false, and model is false/inaccurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When are true positives and false positives important?\n",
    "\n",
    "## Threshold\n",
    "\n",
    "We haven't done anything with the threshold yet. By default scikit-learn uses 0.5 in *predict*, but how should we pick the value for this?\n",
    "\n",
    "We've already got a confusion matrix - we can take the ratio of the true_positive vs the false_positive rates and compare the two "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = np.diag(confusion)\n",
    "incorrect = np.diag(np.roll(confusion, 1, axis=1))\n",
    "correct, incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`total correct` is the number of samples the model predicted an accurate category, `total_incorrect` is the number of samples the model predicted incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_correct = correct.sum()\n",
    "total_incorrect = incorrect.sum()\n",
    "total_correct, total_incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By choosing a different probability threshold, the `total_correct` and `total_incorrect` will change. So will the true positives, true negatives, false positives and false negatives. \n",
    "\n",
    "Let's try changing the threshold, and plotting the results. We need a way of adjusting the class weights in the model. Scikit-learn doesn't let you specify a threshold directly but we can calculate it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_with_class_threshold(threshold):\n",
    "    \"Fit a logistic regression to get an ROC value for a given threshold\"\n",
    "    # Transform our threshold into class weights\n",
    "    class_weights = {True: threshold, False: 1 - threshold}\n",
    "    \n",
    "    # Make a regressor\n",
    "    regressor = pipeline.Pipeline([\n",
    "        ('transform', preprocessing.FunctionTransformer(np.log, validate=True)),\n",
    "        ('model', linear_model.LogisticRegression(class_weight=class_weights))\n",
    "    ])\n",
    "    \n",
    "    # Fit it\n",
    "    regressor.fit(X_train, y_train)\n",
    "    \n",
    "    # Make some predictions, see how we did\n",
    "    results = pd.DataFrame({\n",
    "        'AL2O3': X_test['AL2O3'],\n",
    "        'is_ore_actual': y_test,\n",
    "        'is_ore_predicted': regressor.predict(X_test),\n",
    "        'count': 1\n",
    "    })\n",
    "    confusion = results.pivot_table(\n",
    "        values='count', \n",
    "        index='is_ore_actual', \n",
    "        columns='is_ore_predicted', \n",
    "        aggfunc='sum').fillna(0)\n",
    "    true_neg, false_neg, false_pos, true_pos = confusion.unstack()\n",
    "    correct = np.diag(confusion)\n",
    "    incorrect = np.diag(np.roll(confusion, 1, axis=1))\n",
    "    \n",
    "    # Return results as a dictionary\n",
    "    return {\n",
    "        'threshold': threshold,\n",
    "        'total_correct': correct.sum(), \n",
    "        'total_incorrect': incorrect.sum(), \n",
    "        'true_negative': true_neg,\n",
    "        'false_negative': false_neg,\n",
    "        'true_positive': true_pos,\n",
    "        'false_positive': false_pos,\n",
    "        'accuracy': correct.sum()/(correct.sum() + incorrect.sum()),\n",
    "        'precision': true_pos/(true_pos + false_pos),\n",
    "        'recall': true_pos/(true_pos + false_neg)\n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two other metrics here:\n",
    "- Precision is the estimated probability that a result randomly selected from the pool of predicted positives is a true positive.\n",
    "- Recall is the estimated probability that a result randomly selected from the pool of true cases is predicted accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate results using our metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_with_class_threshold(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_with_class_threshold(0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate over all our thresholds and see what does the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame.from_records(\n",
    "    [fit_with_class_threshold(t) for t in np.linspace(0.1, 0.9)],\n",
    "    index='threshold'\n",
    ")\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A high accuracy means few labels were incorrectly predicted. It is not always a good indicator of the quality of a binary classification since it doesn't account for any imbalance between the categories.\n",
    "\n",
    "That's why precision and recall are always measured as well. A high precision means a low false positive rate, a high recall means a low false negative rate. As scikit-learn [puts it](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html):\n",
    "\n",
    "\"A system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the [test] labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the [test] labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot(y=['total_correct', 'total_incorrect'])\n",
    "results.plot(y=['false_negative', 'true_negative'])\n",
    "results.plot(y=['false_positive', 'true_positive'])\n",
    "results.plot(y=['accuracy', 'precision', 'recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn implements a lot of those metrics already, see for instance:\n",
    "\n",
    "* [Confusion matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)\n",
    "* [Accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)\n",
    "* [Precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score)\n",
    "* [Recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n",
    "\n",
    "Another useful tool to assess the performance of a classifier is the Receiver Operating Characteristic (ROC) metric. It is a curve showing false positive rate vs. true positive rate. Ideally, the curve for the classifier should be as close as possible to the top-left corner (i.e., lots of true positive and few false positive). The diagonal line basically means a random classifier. Measuring the area below the curve turns the curve into a single value: close to 1 is better, 0.5 indicates a random classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, regressor.predict_proba(X_test)[:, 1])\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)'%roc_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension: Modelling with statsmodels\n",
    "\n",
    "For what it's worth we can also generate these using [statsmodels](https://www.statsmodels.org/stable/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to handle preprocessing ourselves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tools import add_constant\n",
    "\n",
    "def preprocess(x):\n",
    "    \"Our preprocessing pipeline for Al2O3\"\n",
    "    return add_constant(np.log(x)) # add_constant adds an intercept to the fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statsmodels uses stats jargon \n",
    "- endog -> endogenous variable -> y\n",
    "- exog -> exogenous variable -> X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endog = df.is_ore\n",
    "exog = preprocess(df['AL2O3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model is pretty similar though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.Logit(endog, exog)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statsmodels does a bit more statistical testing/automated confidence intervals for us at the cost of having to manage crossvalidation etc ourselves. Depending on what you're trying to achieve this could be a viable way to go.\n",
    "\n",
    "As before we can immediately make some predictions - statsmodels gives us the value of the logistic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(results.params, preprocess(X_test['AL2O3']))\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and compare to scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.predict(X_test[['AL2O3']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare the logistic values get slightly different answers here - probably down to the solver used under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' statsmodels:', model.predict(results.params, preprocess(X_test['AL2O3'])))\n",
    "print('scikit-learn:', regressor.predict_proba(X_test[['AL2O3']])[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate the same plot again though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1)\n",
    "\n",
    "# Some aluminium values to predict from\n",
    "al_compositions = np.linspace(0.1, 3)\n",
    "\n",
    "# An offset to stop everything plotting on top of everything else\n",
    "offset = 0.02\n",
    "\n",
    "# shows predictions given contents\n",
    "predictions = model.predict(results.params, preprocess(al_compositions)) > 0.5\n",
    "ax.plot(al_compositions, predictions + offset, '.', alpha=0.7, label='predicted (+ offset)')  \n",
    "\n",
    "# shows measured values plus jitter\n",
    "jitter = np.random.normal(scale=0.01, size=len(df))\n",
    "ax.plot(df['AL2O3'], df['is_ore'] + jitter - offset, '.', alpha=0.1, label='measured (+ jitter - offset)')\n",
    "\n",
    "# shows logistic function fitted from regressor\n",
    "ax.plot(al_compositions, model.predict(results.params, preprocess(al_compositions)), '--', label='fitted logistic function')\n",
    "\n",
    "# Generate the logistic curve showing the location of \n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.legend()\n",
    "ax.set_title('Logistic regression with statsmodels')\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Importance of validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we want to see the importance of splitting our dataset into a training set and a test set. To make things easier to understand, we're gonna work with synthetic data generated using NumPy.\n",
    "\n",
    "First, we need to make our results reproducible by setting a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate the synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things a little more interesting, we're gonna use a sinusoidal function with some random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 120\n",
    "x = np.random.uniform(0, 1, n_samples)\n",
    "noise = np.random.normal(0, 0.1, n_samples)\n",
    "y = np.sin(np.pi*x) + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=x, y=y, fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fit a polynomial model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a bell shape like that a simple linear model won't do, we need a polynomial model. To do that in scikit-learn, we build a pipeline combining *PolynomialFeatures*, which defines the degree of the polynomial, with *LinearRegression* to perform the actual regression.\n",
    "\n",
    "Fill the pipeline to get a polynomial model of degree 20 (let's start high, because more degrees means a more complex model and that's always a good start, right?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(20), LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(0, 1, 1000)\n",
    "y_plot = model.predict(x_plot.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.regplot(x=x, y=y, fit_reg=False, ax=ax)\n",
    "sns.lineplot(x=x_plot, y=y_plot, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And at how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model fit has too many wiggles in its attempt to fit all the data, including all the noise, or scatter. A 20 degree polynomial is too high. This model is **over-fitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Check a model with fewer degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(0, 1, 1000)\n",
    "y_plot = model.predict(x_plot.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.regplot(x=x, y=y, fit_reg=False, ax=ax)\n",
    "sns.lineplot(x=x_plot, y=y_plot, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's really bad... We're not capturing the bell shape at all, and the score and error show it clearly. The problem is that our data are clearly non-linear, and a linear model is not capable of capturing this pattern. So our model is **under-fitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check again with the lowest degree capable of capturing this pattern: 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a polynomial of degree 2 using make_pipeline(), PolynomialFeatures(), and LinearRegression\n",
    "model = \n",
    "\n",
    "# Fit the model using model.fit(...)\n",
    "\n",
    "\n",
    "# Generate some linearly spaced points\n",
    "x_plot = \n",
    "\n",
    "# Predict on these new data\n",
    "y_plot = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.regplot(x=x, y=y, fit_reg=False, ax=ax)\n",
    "sns.lineplot(x=x_plot, y=y_plot, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate model score using model.score()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate mean squared error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better than the linear model, but worse than a degree of 20, so a high degree must be better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 4: Use a test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, maybe we shouldn't train and test on the same data. Let's check the impact of that. With a test set, we should be able to see the over-fitting not just visually but also in the test results.\n",
    "\n",
    "First, we need to split our dataset into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what happens with a degree of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(20), LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train.reshape(-1, 1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(0, 1, 1000)\n",
    "y_plot = model.predict(x_plot.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.regplot(x=x, y=y, fit_reg=False, ax=ax)\n",
    "sns.lineplot(x=x_plot, y=y_plot, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x_test.reshape(-1, 1), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now with a degree of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a polynomial model of degree 2 \n",
    "model = \n",
    "\n",
    "# Fit a model using training data only\n",
    "\n",
    "# Generate linearly spaced data points\n",
    "x_plot =\n",
    "# Predict on these new data\n",
    "y_plot ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.regplot(x=x, y=y, fit_reg=False, ax=ax)\n",
    "sns.lineplot(x=x_plot, y=y_plot, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate model score on test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean squared error for test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, our validation metrics are better with the lower degree now... But that was only one possible splitting, and we don't have so many data, maybe testing it again will lead to different results.\n",
    "\n",
    "Instead of doing it manually, let's use scikit-learn's cross-validation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(20), LinearRegression())\n",
    "score = cross_val_score(model, x.reshape(-1, 1), y, scoring='neg_mean_squared_error')\n",
    "-score.mean(), score.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(2), LinearRegression())\n",
    "score = cross_val_score(model, x.reshape(-1, 1), y, scoring='neg_mean_squared_error')\n",
    "-score.mean(), score.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a degree 2 is clearly a better choice. The problem is that a more complex model can fit more complex patterns, and it's trying to identify patterns in the noise as well. Our model with a degree of 20 is **over-fitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Choose the right degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The now the question become how to choose the best degree for the problem?\n",
    "\n",
    "One easy way to identify that degree is to add the cross-validation into a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = list(range(1, 31))\n",
    "n_cv = 100\n",
    "scores = []\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    score = cross_val_score(model, x.reshape(-1, 1), y, scoring='neg_mean_squared_error', cv=n_cv)\n",
    "    scores.append(-score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=degrees, y=scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at the degree with the smallest error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees[np.argmin(scores)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out scikit-learn can look for and give us directly the best model. To do so, we just need to perform a grid search over the parameter *degree* of our pipeline. For each value of degree we want to test, scikit-learn is gonna perform cross-validations with a given score, and use that to pick the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=[('polynomial', PolynomialFeatures()),\n",
    "                        ('regression', LinearRegression())])\n",
    "param_grid = [{'polynomial__degree': degrees}]\n",
    "scoring = 'neg_mean_squared_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(model, param_grid, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.fit(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the search led to the right degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can directly access the model to make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(0, 1, 1000)\n",
    "y_plot = search.best_estimator_.predict(x_plot.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.regplot(x=x, y=y, fit_reg=False, ax=ax)\n",
    "sns.lineplot(x=x_plot, y=y_plot, ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "core05",
   "language": "python",
   "name": "core05"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
