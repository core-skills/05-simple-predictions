{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Explore a dataset using pandas and seaborn\n",
    "\n",
    "## Pandas and seaborn Refresher\n",
    "\n",
    "Let's review using Seaborn and Pandas to load up some data and then pair plot it.\n",
    "\n",
    "We'll be using the same tools that we used last week for this \n",
    "- [pandas](pandas.pydata.org) for data handling (our dataframe library)\n",
    "- [seaborn](seaborn.pydata.org) for _nice_ data visualization\n",
    "\n",
    "Shortly we'll also by trying out:\n",
    "\n",
    "- [scikit-learn](scikit-learn.org) an extensive machine learning library.\n",
    "- [numpy](numpy.org) - a fundamental maths library best used by people with a strong maths background.  We won't explore it much today, but it does have some useful methods that we'll need.  It underlies all other mathematical and plotting tools that we use in Python.\n",
    "\n",
    "We'll be using scikit-learn over the next few weeks, and it's well worth reading the documentation and high level descriptions.\n",
    "\n",
    "_You will probably want to take a moment to look at the documentation of the libraries above - especially pandas_\n",
    "\n",
    "The other useful resource is Stack Overflow - if you have a question that sounds like 'how do I do {x}' then someone will probably have answered it on SO. Questions are also tagged by library so if you have a particular pandas question you can do something like going to https://stackoverflow.com/questions/tagged/pandas (just replace the 'pandas' in the URL with whatever library you're trying to use.\n",
    "\n",
    "Generally answers on SO are probably a lot closer to getting you up and running than the documentation. Once you get used to the library then the documentation is generally a quicker reference. We will cover strategies for getting help in class.\n",
    "\n",
    "## Git links\n",
    "\n",
    "If you want to work in pairs, use GitHub and GitKraken to share code. Here are some useful links for reference:\n",
    "\n",
    "- GitKraken interface basics: https://support.gitkraken.com/start-here/interface\n",
    "- Staging and committing (save current state -> local history): https://support.gitkraken.com/working-with-commits/commits\n",
    "- Pushing and pulling (sync local history <-> GitHub history): https://support.gitkraken.com/working-with-repositories/pushing-and-pulling\n",
    "- Forking and pull requests (request to sync your GitHub history <-> someone else's history - requires a _review_):\n",
    "  - https://help.github.com/articles/about-forks/\n",
    "  - https://help.github.com/articles/creating-a-pull-request-from-a-fork/\n",
    "\n",
    "## Step 1: Read in the dataset\n",
    "\n",
    "For this exercise, we will be using the Beijing PM2.5 Data Set, which contains meteorological data from Beijing Capital International Airport and atmospheric particulate matter (PM) that have a diameter of less than 2.5 micrometers.\n",
    "\n",
    "All the packages you need for the exercise are already there, just run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if using jupyterhub\n",
    "# %pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to download the data into the folder *data*. Define *filename*, which provide the path to the folder *data* and the name of the file (you can use the same name as in the URL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename =\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00381/PRSA_data_2010.1.1-2014.12.31.csv'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the file using pandas and *filename*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the content of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the summary statistics of each variable using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use seaborn to plot a pairplot of all the variables in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Focus on the variables of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is slightly biased, because our goal is to perform a linear regression, and it turns out two variables show a nice linear relationship.\n",
    "\n",
    "Find those two variables and plot their distributions using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot the variation of one variable compared to the other using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas or seaborn, check the correlation between those two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Find a linear regression with Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've seen a linear relationship between two of the variables, use Seaborn to plot the line of best fit.\n",
    "\n",
    "There are a few different ways to do this. Try using regplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Linear regression with scikit-learn\n",
    "\n",
    "Scikit-learn provides machine learning tools in several categories. These include supervised learning and unsupervised learning. We'll start working with unsupervised learning next week. Supervised learning is about finding a model for features that can be measured and some labelling that we have for the available data. If, for example, we have lithium assays and we want to try to predict lithium based on sensor data from a portable spectrometer, then the lithium assays are the labels and the measured intensities at different wavelengths are the measured features. This kind of supervised learning is called regression.\n",
    "\n",
    "There's another kind of supervised learned which is called classification, this is what we're doing when we want to assign observed data to different discrete classes. Regression can sometimes be used, with minor additions, to classify data as well. For example, with our lithium spectral regression model we could classify samples as being high in lithium or low in lithium simply by using a threshold value that we set. There are more sophisticated ways to classify, which will be covered in later weeks.\n",
    "\n",
    "We use the estimator API of scikit-learn to do regression.\n",
    "\n",
    "## The Estimator API of scikit-learn\n",
    "\n",
    "There are a few steps to follow when using the estimator API.  These steps are the same for all methods that scikit-learn implements, not just for linear regression.\n",
    "\n",
    "1. Choose a class of model by importing the appropriate estimator class. In our case we want to import Linear Regression. Scikit-learn's documentation might come in handy for that.\n",
    "\n",
    "First, import LinearRegression from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create an \"instance\" of the LinearRegression class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that this has worked look at the model object after it's created. It should tell you about some of its settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These settings are also called hyperparameters.  We'll encounter hyperparameters again next week, and will talk about them in more detail then.  They're often very important in working out whether our model is well fitted to the data.\n",
    "\n",
    "2. Next we need to arrange a pandas dataframe into a features matrix and a target vector.\n",
    "\n",
    "Search on the Internet for this, and use the two variables identified during the previous exercise.  I know that Stack Overflow will be helpful.  You will need to look at the column names in the dataframe to find the names of the two columns that are important to us.  Do this in the next cell.\n",
    "\n",
    "The notation is a bit strange!  The two pairs of \"[ ]\" as \"[[ ]]\" that you will see is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \n",
    "y = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Fit the model to your data by using the fit() method of the LinearRegression object.\n",
    "\n",
    "Again, look at the documentation for how to apply this.  You'll need to provide your features matrix (X) and target vector (y) as parameters to the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congratulations you've trained your first machine learning model!\n",
    "\n",
    "As this is a two dimensional linear model, it has two parameters.  The line's intercept and slope.  The notation that scikit-learn uses is a little unfriendly.  Its convention is to add underscores to the names of the parameters it finds.  Also, it calls the slope \"coef\".\n",
    "\n",
    "After fitting the model, find the coefficient and intercept of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also look at the coefficient of determination of the model, R<sup>2</sup>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we've trained a model, we should make predictions!\n",
    "\n",
    "6. Make predictions!\n",
    "\n",
    "This is also more complicated with scikit-learn than it is with Seaborn.\n",
    "\n",
    "For a given, single value for a feature (i.e., a temperature) we can predict a label.  For example, for a temperature of 20 &deg;C, we could make a prediction with:\n",
    "\n",
    "```predicted_pressure = model.predict(20)```\n",
    "\n",
    "But to find the smooth line that seaborn finds we need to explicitly tell scikit-learn that we want to do a prediction for all of the temperatures that we're interested in. To do this we\n",
    "use a new library called \"numpy\" and a method called linspace (which is short for linear spacing).\n",
    "\n",
    "First we need to import numpy.\n",
    "\n",
    "```import numpy as np```\n",
    "\n",
    "While I used predicted_pressure above as an example of a predicted target array, and 20 is an example of x, I'll now switch to the usual y and x conventions used in tutorials with scikit-learn.  You can of course use any variables names you, and in your own code it's best to use descriptive names that mean something in the domain of your industry, like 'predicted_pressure\", or \"octane_rating\".\n",
    "\n",
    "We need to use the linspace method in numpy.  Use it like this:\n",
    "\n",
    "```x_fit = np.linspace(-20, 40)```\n",
    "\n",
    "This will create a collection of temperatures, in order, starting from -20 &deg;C up to 40 &deg;C.  This is what we need, but this collection isn't formatted correctly for scikit-learn.  To make it work with scikit-learn we next have to adjust the format with this instruction:\n",
    "\n",
    "```x_fit_reshaped = x_fit[:, np.newaxis]\n",
    "y_fit = model.predict(x_fit_reshaped).```\n",
    "\n",
    "y_fit now contains our predicted pressures.  Type ```y_fit``` to see them numerically.\n",
    "\n",
    "Try this all out in the next cell.  Take it step by step.  Don't try to run this all in one go, but build it up line by line, checking that you do not get errors after each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although pandas and seaborn work nicely for simple plots, we need sometimes to go back to matplotlib, which they both use in the background. Here we do that to reproduce the result we got from regplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "df.plot(x='TEMP', y='PRES', kind='scatter', alpha=0.15, ax=ax)\n",
    "ax.plot(x_fit, y_fit, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also predict our training data, to compare the predicted *y* to the real *y* from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compute the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use seaborn to plot the distribution of those residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Perturbing perfect linear data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we want to look at the influence of noise and outliers on the predictions of a linear regression. To make things easier to understand, we're gonna work with synthetic data this time.\n",
    "\n",
    "Scikit-learn has a set of functions to generate synthetic data. You can find more information about them here:\n",
    "\n",
    "[scikit-learn.org/stable/datasets/index.html#sample-generators](https://scikit-learn.org/stable/datasets/index.html#sample-generators)\n",
    "\n",
    "You can start playing with the generators for regression at the end of this exercise if you want, but in the meantime we're gonna use a lower-level approach with NumPy, which gives us more flexibility.\n",
    "\n",
    "NumPy is a collection of mathematics functions which underlies all other mathematical libraries that we've been using, such as Seaborn and scikit-learn. *random* is a NumPy's module to generate random numbers from distributions.\n",
    "\n",
    "First, we need to set up the seed, which means that our results will be reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate a perfect linear data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a simple linear dataset using NumPy. A uniform distribution means that all of the values that may be returned are equally likely.  When we throw dice we are sampling from a uniform distribution.\n",
    "\n",
    "Tell Python that for *x* we want random numbers between 0 and 100 from a uniform distribution, and we want *n_samples* of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "x = \n",
    "a = 0.75\n",
    "b = 0.75\n",
    "y = a*x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.regplot(x=x, y=y, line_kws={\"color\": \"red\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn is very powerful, but it can be a bit long to set up, especially for a problem as simple as this one. Fortunately Python offers other solutions. One is [statsmodels](https://www.statsmodels.org/stable/index.html), a module \"that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration.\"\n",
    "\n",
    "Here we're gonna use another widely used package for scientific computing, [SciPy](https://www.scipy.org/), and its [stats](https://docs.scipy.org/doc/scipy/reference/tutorial/stats.html) module in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the correlation coefficient (or Pearson coefficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pearsonr(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can fit a linear model to the data and look at a (the slope), b (the intercept), and R<sup>2</sup>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.linregress(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add Gaussian noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had a perfect linear relationship, let's add some noise. A normal (or Gaussian) distribution returns values which are most likely to be near the mean, falling off symmetrically to either side.  It is the \"bell\" curve that you've seen many times.\n",
    "\n",
    "Here, tell Python that we want the noise that we add to our simple line to have a mean of zero, and a standard deviation of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = \n",
    "y = a*x + b + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.regplot(x=x, y=y, line_kws={\"color\": \"red\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pearsonr(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.linregress(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use eaborn's residplot function to plot the residuals after fitting a line to the data. With a normal distribution we expect to see these residuals evenly scattered around zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to run the code again with different number of samples and different level of noise. What happens?\n",
    "\n",
    "## Step 3: Add non-Gaussian noise\n",
    "\n",
    "Now let's see what happens when the noise isn't normally distributed. An example of a heavy tailed distribution is the gamma distribution.  This is often used to model failure likelihood for machines.  Unlike the normal distribution it is not symmetric.  In quality control applications it quickly peaks after a short lifetime, but then has a long tail that extends many years into the future.  This makes sense as we expect most failures to be early in the life of a machine because of manufacturing faults, after that the failure time is less predictable, but we all know of machines or gadgets that seem to last forever.  Google will quickly bring up examples of the shape.\n",
    "\n",
    "Tell Python that now we want the error to follow a gamma distribution of parameters k = 2 and theta = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = \n",
    "y = a*x + b + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot the data and regression line with seaborn, look at the correlation coefficient and R<sup>2</sup> with stats, and the residuals with seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Add outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start again with a normally distributed noise in *y*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outliers = 5\n",
    "x_outliers = np.random.uniform(0, 40, n_outliers)\n",
    "y_outliers = np.random.uniform(80, 100, n_outliers)\n",
    "\n",
    "x_outliers = np.concatenate((x, x_outliers))\n",
    "y_outliers = np.concatenate((y, y_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data and regression line with seaborn, look at the correlation coefficient and R<sup>2</sup> with stats, and the residuals with seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to change the number of outliers and their distribution for *x* and *y*, and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Step 5, Add a second population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add another population that follows a linear relationship between *x* and *y* too, but with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_spop = 100\n",
    "a_spop = 1\n",
    "b_spop = 2\n",
    "noise_level_spop = 1\n",
    "x_spop = np.random.uniform(80, 100, n_spop)\n",
    "noise_spop = np.random.normal(0, noise_level_spop, n_spop)\n",
    "y_spop = a_spop*x_spop + b_spop + noise_spop\n",
    "\n",
    "x_spop = np.concatenate((x, x_spop))\n",
    "y_spop = np.concatenate((y, y_spop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data and regression line with seaborn, look at the correlation coefficient and R<sup>2</sup> with stats, and the residuals with seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Step 6, Add heteroscadistic error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how would you change this code to create a heteroscadistic error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data and regression line with seaborn, look at the correlation coefficient and R<sup>2</sup> with stats, and the residuals with seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
